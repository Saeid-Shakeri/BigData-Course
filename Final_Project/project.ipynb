{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to access local files (e.g., the PDF)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install langchain sentence-transformers faiss-cpu pdfplumber langchain-community langchain-ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Imports ----------------------\n",
    "import pdfplumber  # For PDF text extraction\n",
    "from langchain_community.vectorstores import FAISS  # FAISS for vector database\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # HuggingFace embeddings for sentence vectors\n",
    "from langchain.chains import ConversationalRetrievalChain  # LangChain RAG Chain with memory\n",
    "from langchain_ollama.llms import OllamaLLM  # Ollama LLM integration (e.g., mistral)\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Custom prompt templates\n",
    "from langchain.memory import ConversationBufferMemory  # Conversation memory for chat history\n",
    "from langchain.docstore.document import Document  # LangChain document object\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)  # Suppress PDFMiner warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Step 1: Extract text from PDF ----------------------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts full text content from all pages of a PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Step 2: Split text into paragraph-based chunks ----------------------\n",
    "def split_into_documents(text):\n",
    "    \"\"\"\n",
    "    Splits the raw text into a list of LangChain Document objects,\n",
    "    each representing a paragraph.\n",
    "    \"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    docs = [Document(page_content=p) for p in paragraphs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Step 3: Build FAISS vector index ----------------------\n",
    "def build_faiss_index(docs):\n",
    "    \"\"\"\n",
    "    Builds a FAISS vector store using HuggingFace sentence embeddings.\n",
    "    \"\"\"\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(docs, embedding=embedder)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Step 4: Create the RAG chain with custom prompt ----------------------\n",
    "def create_conversational_rag_chain(vectorstore):\n",
    "    \"\"\"\n",
    "    Creates a Conversational Retrieval-Augmented Generation (RAG) chain\n",
    "    using Ollama's Mistral model and a custom prompt template.\n",
    "    \"\"\"\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    model = OllamaLLM(model=\"mistral\")\n",
    "\n",
    "    # Initialize memory to maintain chat history\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Define custom prompt template with context, history, and question\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are a helpful AI assistant expert in Docker, Dockerfiles, and dockercompose.\n",
    "\n",
    "Here is the conversation history:\n",
    "{chat_history}\n",
    "\n",
    "Relevant context from documents:\n",
    "{context}\n",
    "\n",
    "Now, answer the user's question as clearly and practically as possible:\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    # Create the final conversational chain\n",
    "    rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Step 5: Prepare PDF and build the chain ----------------------\n",
    "# Path to your PDF in Google Drive\n",
    "pdf_path = \"/content/drive/MyDrive/LearningDocker.pdf\"\n",
    "\n",
    "# Pipeline: extract -> split -> embed -> retrieve -> generate\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "docs = split_into_documents(text)\n",
    "vectorstore = build_faiss_index(docs)\n",
    "rag_chain = create_conversational_rag_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------- Step 6: Start interactive chat loop ----------------------\n",
    "print(\"âœ… RAG Agent is ready. Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"ðŸ’¬ Your question: \")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"ðŸ‘‹ Goodbye!\")\n",
    "        break\n",
    "    response = rag_chain.run(question)  \n",
    "    print(\"ðŸ”Ž Answer:\\n\", response, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
